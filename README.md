# Personalized-GPT2-Text-Generator
# GPT-2 Fine-Tuning for Custom Text Generation

This project fine-tunes OpenAI's GPT-2 model on a custom dataset to generate coherent and contextually relevant text based on a given prompt.

## ğŸš€ Features
- Fine-tunes GPT-2 on any custom text dataset.
- Supports training and evaluation with `Trainer` API.
- Generates high-quality text based on prompts.
- Saves and loads fine-tuned models for future use.

## ğŸ“‚ Project Structure
ğŸ“‚ GPT2-Fine-Tuning â”œâ”€â”€ ğŸ“œ data.txt # Custom training dataset â”œâ”€â”€ ğŸ“œ train_gpt2.py # Main script for fine-tuning GPT-2 â”œâ”€â”€ ğŸ“‚ gpt2-finetuned # Folder to store fine-tuned model â”œâ”€â”€ ğŸ“‚ logs # Training logs â”œâ”€â”€ ğŸ“œ README.md # Project documentation â””â”€â”€ ğŸ“œ requirements.txt # Required dependencies

## ğŸ›  Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/GPT2-Fine-Tuning.git
cd GPT2-Fine-Tuning
2ï¸âƒ£ Install Dependencies
Ensure you have Python 3.8+ installed, then install dependencies:
pip install -r requirements.txt
3ï¸âƒ£ Download GPT-2 Model
The script automatically downloads GPT-2 from Hugging Face, but you can manually download it:
from transformers import GPT2Tokenizer, GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
ğŸ‹ï¸ Fine-Tune GPT-2
Run the following command to train the model on your dataset:
python train_gpt2.py

ğŸ”® Generate Text with the Fine-Tuned Model
After training, test the model using:
from transformers import pipeline

generator = pipeline("text-generation", model="./gpt2-finetuned", tokenizer="./gpt2-finetuned")
prompt = "Once upon a time"
result = generator(prompt, max_length=100, num_return_sequences=1)
print(result[0]['generated_text'])

ğŸ“ Customizing the Dataset
Modify data.txt with your own text to train GPT-2 on different writing styles.

ğŸ›  Troubleshooting
FileNotFoundError: Ensure data.txt exists in the correct path.
Tokenizer Padding Error: Run tokenizer.pad_token = tokenizer.eos_token before training.
CUDA Memory Issues: Try reducing per_device_train_batch_size.
ğŸ“œ License
This project is licensed under the MIT License.

â­ Acknowledgments
Hugging Face Transformers
OpenAI's GPT-2
ğŸ”¥ Contribute & Feedback
Want to improve this project? Feel free to fork, open an issue, or submit a pull request!

If you found this useful, give it a star â­ on GitHub!
